{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=E2shqsYwxck\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='It is subjective to determine the best cricket team in the world as rankings and performance can vary over time. However, some of the top cricket teams currently are India, Australia, England, and New Zealand.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Which is the best cricket team in the world?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The problem that rag (retrieval-augmented generation) solves in AI is the issue of generating more coherent and contextually relevant responses in conversational systems. Traditional language models often struggle with understanding and maintaining context over a longer conversation, leading to generic or off-topic responses. \\n\\nRAG addresses this problem by first retrieving relevant information from a large pre-existing knowledge base and then using this information to guide the generation of a more accurate and contextually appropriate response. This approach helps improve the overall quality and relevance of the generated responses in AI systems, making them more effective in natural language processing tasks.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "\n",
    "chain.invoke(\"What is the problem rag solves in ai ? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based on the context below. If you cant \\nanswer the question, reply \"I dont know\"\\n\\nContext: Mary\\'s sister is Susana\\n\\nQuestion: Who is Mary\\'s Daughter?\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you cant \n",
    "answer the question, reply \"I dont know\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's Daughter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Susana'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\" : \"Mary's sister is Susana\",\n",
    "    \"question\": \"Who is Mary's sister?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'मेरी के दो भाई बहन हैं।'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate {answer} to {language}\"\n",
    ")\n",
    "\n",
    "translation_chain = (\n",
    "    {\"answer\" : chain, \"language\" : itemgetter(\"language\")} | translation_prompt | model | parser\n",
    ")\n",
    "\n",
    "translation_chain.invoke (\n",
    "    {\n",
    "    \"context\" : \"Mary's sister is Susana and her brother name is raul\",\n",
    "    \"question\": \"How many siblings does mary have?\",\n",
    "    \"language\" : \"Hindi\",\n",
    "}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube\n",
    "\n",
    "\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "        \n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Hi, this is Lance from the chain team. I'm going to talk about building a self-reflective rag apps from scratch using only open source and local models that run strictly on my laptop. Now one of those interesting trends in the rag research and a lot of methods that become pretty popular in recent months and weeks is this idea of self-reflection. So when you do rag, you perform retrieval based upon a question from an index. And this idea of self-reflection is saying based upon, for example, the relevance of the tree documents to my question or based upon the generations relative to my question or the generations relative to the documents, I want to make, I want to perform some kind of reasoning and potentially feed back and retry various steps. So that's kind of the big idea. And there's a few really interesting papers that implement this. And what I want to kind of show is that implementing these ideas using something that we've developed recently called LandGraph is a really nice approach and it works really well of local LMS that are much smaller, for example, than API-gated, very large scale foundation models. And so we're going to look at one particular paper called Corrective Rag or Sea Rag. Now this paper is kind of, there's been some attention on, for example, Twitter about this work. It's a really neat paper. And the idea is actually pretty simple and straightforward. If you go down to the figure here, you're going to do, perform a treeval and you're going to grade the documents relative to the question. So you're kind of doing a relevance grading. And there's some heuristics, like basically if the documents are deemed correct, they actually do some knowledge refinement where they further strip the documents to compress relevant chunks within the documents and retain them. And if the documents are either deemed ambiguous relative to the query or incorrect, it performs a web search and supplements retrieval with a web search. So that's kind of a big idea. But it's a nice illustration of this general principle of don't just do rag as kind of like a single shot process for your performer treeval and then go to generation. You can actually perform self-reflection or reasoning. You can retry. You can retrieve from alternative sources and so forth. That's kind of a big idea. Now in our build here, we're going to make some minor simplifications. Here's kind of a layout of the graph that we're interested in. We're going to perform a treeval and for that we're going to use no-mic embeddings which run locally. We're going to build a node for grading. Those documents relative to the question to say are they relevant or not. And if any documents are deemed irrelevant, we'll go ahead and do a query rewrite web search, embles, then go ahead to generation based on the web search result. So that's the flow. Now first things first is how do I get started running LMS locally and kind of where do I go. And where I often direct people and what I found to be really useful is Olamah. It is a really nice way to run models locally, for example on your Mac laptop very easily and they are launching support for various other platforms as well. And so basically if you go to their website, it's very simple. You simply download their application. You can see it's running here on my machine. And once you have it downloaded, all you need to do is you can go to their model list and you can kind of search around. So you can actually look, I think it's sorted by popularity. You can see Mistral, obviously a really interesting open source model is kind of one of the top. So you can see it has like 210,000 polls. It's one of the top models. I click on this. And where this takes me is a model page. I can look at this tags tab. And this basically shows me a bunch of model versions that I can really easily just like download and run. And we'll show how to do that here very shortly. What I'm going to do is I'm going to choose Mistral and struct. So that is there's 7 billion parameter and struct model. And so all I would do, I'm going to go over to my notebook here. So I have an empty notebook. And all I've done is I've already done a few pip installs. And I've also set a few environment variables to use Langsmith. And we'll see why that's useful later. That's really all I've done. Now I'm going to put a note here to, for Olamma. And what I'm going to do is this Olamma pull the model I want and use run that. So normally this will take a little bit because you're actually pulling the model. And typically it's like a couple gigs. I actually already have this model. So it's faster. It's actually already done. But that's really all you do. Okay, so that's kind of like step one. And then what we're going to do is I'm just going to create this variable local LLM. That I am going to yeah, so I'm just going to define this variable. Mistral and struct because this is the model that I download using Alama pull Mistral struct, that's all that's going on here. So this is going to be the LLM I'm going to work with. I've pulled this so it's local on my system. It's available via Olamma, which is basically running the background on my system. And you can see it's really seamless and easy to use. Now the first thing I want to do for this approach is I'm going to call this index. So because this was a corrective rag approach, I need an index that I care about that I'm actually performing rag on. And so here I'm going to use a particular blog post that I like on agents. We can like pull it up here and have a look. Let's pull it up over here actually. So this is a pretty neat blog post on autonomous agents. It's like pretty long and meaty. So it's kind of like a good target for performing retrieval on. It's a details here, really neat, really detailed blog post. So what I'm going to do is I'm going to load it here. I'm going to split it and I'm going to use a chunk size of 500 tokens. These are kind of somewhat arbitrary parameters. You can play with these as you want. The point is here I'm just building up a quick local index. So I load it. I split it into chunks. Now this is the interesting bit. I'm going to use GPT for all embeddings from NOMIC, which is, let's actually pull up the link here. I had it available here. So these are, you can see right here, it is a CPU optimized contractively trained as basically SBIRT model. So you can like join the sentence transformers so you can see. Yep. So this is the initial work describing our paper, SBIRT basically. So the key point is this. This is a locally running CPU optimized embedding model that works quite well. I found runs on your system, no API, nothing. So it's pretty nice. Runs fast. So we're going to go ahead and use that from our friends at NOMIC. And I'm also going to use Chrome, which is an open source local vector store that's really easy to spin up runs locally. And all I'm doing is I'm taking my documents, I'm going to define a new collection, taking my embedding model, GPT for all embeddings, I'm going to create a retriever from this. So there we go. Okay, it shows you some parameters. So cool, I have a retriever. So we can actually call, we can say get relevant documents. And I can say something about like, let's say like agent memory or something. You know, let's just test. And okay, cool, look at that. So it's nice and quick. We get a bunch of documents out that relate to memory. So you can see memory stream, like the documents are are sane. So it looks like everything's kind of working here. So that's great. We have a retriever. Now let's think a little bit about what we want to do next. So when I do these kinds of kind of logical rag flows, but as graphs, first I always try to lay out the logic and move this up here. I try to lay out kind of the logical steps. And in each logical step, what's happening is I'm transforming a state. So in these graphs, really all you're doing is you're defining a state that you're just modifying throughout the flow of the graph. Now in this case, because we're interested in rag, our state is just going to be a dictionary. And that dictionary you can see, I actually kind of schematically laid it out here. It's just going to contain a few keys that are things relevant to rag. It's going to be like a question. Then it's going to be you append documents to your dict and then eventually you're going to append generation. So that's really all that's going on in terms of like how your state's being propagated through the graph. And at every node, you're making some modification to state. That's the key point. So you're basically going to do you start with a question from the user, you perform retrieval relevant to the question, you're they're going to grade the documents. So you're going to do a modification of the documents. Then you're going to make a decision, are they relevant or not? If they're not relevant, you're going to transform the query, modify the question, do a web search. The final step is a generation based on the tree of documents. So that's your flow. Now what I want to call out here is there's one very important what we call conditional edge where depending upon the results of the grading step, I want to do one thing or another. So I'm going to make a decision. So I want to show you something that's very convenient that we can use with Olamma to help us here. So this is, I'm going to make a note here to note what I'm going to highlight. So this is Olamma JSON mode. So the basic logic of that conditional edge decided generate is going to be something like this. I already have this prompt laid out. But it's basically going to be, I'm going to take a document and I'm going to take my question. And I'm going to do some kind of comparison to say is the document relevant to the question. That's really what I want to do. But here's the catch. Because I want that edge to process a very particular output, either yes or no, I want to make sure that my output is structured in a way that can reliably be interpreted downstream in my graph. This is where JSON mode from Olamma is really useful. And you can see all I do is now I'm importing shadow Lama. This is going to reference that local model that I specified up here, Mr. Onstruct, which I've downloaded. So I have the model locally. And I'm just flagging this format JSON to tell the model to output JSON specifically. And what I'm going to do in my prompt here, I'm basically saying, you know, you're a greater. Here's the documents, here's the question. And here's the catch. Have a binary score yes, no, and provide it as JSON with a single key score and no preamble no explanation. So I kind of explain in the prompt what I want. And when I call this with JSON mode, it will enforce that JSON is returned. And hopefully with a single key, we expect score and either binary yes, no. And when I'm going to run that as a chain, so I'm going to supply that prompt to my L, and I'm going to then parse that JSON string out into a JSON object, which then I can work with. So let's try that. We're going to try to run this chain we defined. We're going to run retrieval on here's a question. Here's our docs. Let's grade one of the docs using basically passing question and one document. And we're going to take the page content from the document, which is like basically all the text. And we're going to run this. So let's test that quickly. And it is still running. Now it's finished. Let's check the output here. And we can see so we get a JSON back, which is just the score. Yes, no. So that's exactly right. That's what we want. And we can actually look under the hood here at, yeah. So we can actually look under the hood in Langsmith at the grading process. And we can see here that our prompt got populated with the context. So here is a document. And right, here is the question. Here is a document. And the task was of course to grade it. So we can see here's like the full prompt. You're greater assessing the raw blood surgery document. Here's a document. And then here's the model output score. Yes. So this is really nice. We basically enforce the output from our local LLM using JSON mode. So we know every time it's going to output binary yes, no score as a JSON object, which we think is an extra. So that's a very key point that I just wanted to flag. It's a very nice thing that Alama offers that's extremely helpful when building out, particularly these kind of logical graphs where you really want to constrain the flow at certain edges. So that's kind of the like really key thing I wanted to highlight. A lot of the rest of this is actually pretty straightforward. So let's now define our graph state. This is the dictionary that we're going to basically pass between our nodes. So this is just some code I'm going to copy over. This is defining your graph state. You're just saying it's a dick. That's all there is really to that. Now here is where I'm going to copy over some code that basically implements a function for every node and every conditional edge in our graph. So if you remember, we can kind of go over and look, our graph is laid out like this. And all we're doing is for every node drawn, we're going to find a corresponding function here, which performs some operation. So retrieve is basically just doing, we had our tree redefined, get relevant documents, and write them out to state. So again, we take a question in. So if you look here, we basically have this state dick passed into the function. We extract the state dick here. We extract the question from the state dick. We do retrieval and we write that state dick back out to the output. So you think about every node is just doing some modification on the state. Doing it in, doing something, writing it back out. That's really all that's going on. And we can really just march across our little diagram here and see how basically each one of these nodes is implemented as a function. And again, you can see in every case, we're using, for example, Chattolama, in some of these cases, we don't need JSON mode. So if we're just doing like a generation step, as you can see here, we don't need JSON mode for the grading we do. So we're actually going to implement here the same thing we just showed. Chattolama using JSON mode. And what's going to happen is we can see we generate our score every time. And then we can extract our grade from that JSON. And then we know the grade is going to constrain to the output yes or no. Then here's the key point. We do some logical reasoning on that to say, for example, if the grade is yes, then we're going to like append the document, it's relevant. If not, then what we're going to do is we're going to filter that document out. And we're also going to set this flag to search, perform web search as yes. So what really happening here is we are kind of applying kind of a logical gate to say, if any document is scored as relevant, then we just add it to our final list of filter documents. If not, we're going to go ahead and do a web search. And we're going to set the search flag to be yes. And we're not going to include that document in the output. And you can see here, we return a dictionary, which contains our filter documents, our question, and then that flag to run web search yes or no. You can see it was default no. But if we ever encounter a new relevant document, we change that to yes. So that's really all that's going on here. You can see we do our query transform down here again. We just use mistrol again. Here is like a transform prompt. But you kind of get the idea. Web search no, we use tably web search here. It's really kind of a nice quick way to perform web searches. And you can see we just supplement the documents with the web search results. And then this was kind of the final step where we wrote out yes or no to our search key. And depending upon the state, which we can read in here, we make decision to basically either return transform query or return generate, which will basically, that's determining the next node to go to. So this decide to generate is our conditional edge. That's actually right here. And so it's looking at the results that we wrote out from great documents in particular that search yes or no key in our dict. And it's then going to basically determine the next node to diverse to that's really all we're doing here. So that's kind of nice. Now what we're going to do is we kind of copied over all these functions. We then can go ahead and run that. And now we just lay out our graph. So again, our graph was kind of explained here. And here's where we actually just lay out the full kind of graph organization. How we're going to connect each node. So we add the nodes first. We set our entry point. And then we add the edges accordingly between the nodes. And basically the logic here just maps over to our diagram here. That's really all that's happening. Cool. So I'm going to go ahead and go down. And now let's kind of see this all working together. So I'm going to go ahead and compile my graph. I'm going to go ahead and ask a question, explain how the different types of agent memory work. And what I'm going to do, let's go back to our diagrams, we can kind of reference that. I'm going to call this. And I'm actually just going to like this will like traverse every step along the way. And it'll print out something to explain what's happening. So you can see a performer tree bowl and now do migrating steps. And this is all running locally. And they were all deemed relevant. So then I'm going to go ahead and generate. And it's running right now. And there we go. So we can go over to Lang Smith. And let's actually have a look at what happened under the hood. Since this is what just ran. So we can see that at each one of these steps we call chat alama with our mistral 7B model that's running locally. And this is our grading step. So this was each document being graded. So again, like look at this. So it outputs a binary score yes, no, as a dict. That's great. So this has a bunch more down here. So these are all of our documents graded. And now here is that final alama call, which basically packed that all into our rag prompt. You're an assistant for question answering tasks. He's a following to answer the question. Here's all of our docs. Here's the answer. So that's pretty cool. We can see that this multi-step logical flow all works. Now let's try something kind of interesting. I'm going to ask a question that I know is not in the context and see if it will kind perform that default to do web search. So I'm going to say explain how alpha codium works. So this is a recent paper that came out that's not relevant at all to this blog post. So I know that retrieval should not be considered relevant. And let's go ahead and run that and convince ourselves that that's true. So good. This is perfect. So the greater is determining these documents are not relevant. And so it should be making the decision to perform web search. So it should be kind of going to this lower branch transform the query run web search. And it looks like that all ran. So it tells us alpha codium is double source AI coding generation tool developed by code M and I. This is perfect. That's exactly what it is. And we can actually go into Langsmith and again see what happened here. So you can see here the trace is a little bit more extensive because all of our grades are incorrect. It's now irrelevant. Again, we get the nice JSON out. And okay. So this is pretty cool. So this was our question rewriting node. So basically provide an improved input question without any preamble. So what is the mechanism behind alpha codium functionally? So it modifies a question. We use to alley search right here. So it basically does retrieval. It searches for stuff related to alpha codium. So that's great. And then we finally passed that to our model for generation based on this new context. And there we go. So the code is almost for say I code assistant tool. So that kind of gives you the main idea. And the key point is this is all running locally. Again, I used GPT for all embeddings for indexing up at the top right here. And I used Olawa with mistral 7B instruct and JSON mode for that one crucial step where I need to constrain the output to be kind of a score of yes, no. And for other things, I just use the model without JSON mode to do perform generations, like to question rewrite or to do the final generation. So in any case, I hope this gives you kind of an overview of how to think about building logical flows. Doesn't have to be ragged. But rag is a really good kind of use case for this using local models and land graph. And the thing I want to kind of leave you with is there is a lot of interest in complex logical reasoning using local elements and a lot of focus on using agents. And I do want to kind of encourage you to think about depending on the problem you're trying to solve, you may or may not actually need an agent. It's possible that kind of implementing a state machine or a graph kind of as shown here with some series of logical steps. This can incorporate cycles or loops back to like prior stages. We have some more complex examples that show that. This actually can work really well with local models because the local model is only performing a step within each node. So you're kind of constraining it to like just do this little thing, just do this little thing, like just rewrite the question, just grade the document rather than using the local LM as like an agent executor that has to make all these decisions kind of jointly or kind of in a less controlled workflow. Where for example, like the ordering of these various tasks can be determined arbitrarily by the agent. Here we really nicely constrain the logical flow and let the local model just do little tasks at each step. And I've just found it to be a lot more reliable and really useful for these kinds of like logical reasoning tasks. So hopefully this is helpful, give it a try and we'll make sure all this code is easily shared. Thank you.\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Hi, this is Lance from the chain team. I'm going to talk about building a self-reflective rag apps from scratch using only open source and local models that run strictly on my laptop. Now one of those interesting trends in the rag research and a lot\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content='research and a lot of methods that become pretty popular in recent months and weeks is this idea of self-reflection. So when you do rag, you perform retrieval based upon a question from an index. And this idea of self-reflection is saying based', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content='is saying based upon, for example, the relevance of the tree documents to my question or based upon the generations relative to my question or the generations relative to the documents, I want to make, I want to perform some kind of reasoning and', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"of reasoning and potentially feed back and retry various steps. So that's kind of the big idea. And there's a few really interesting papers that implement this. And what I want to kind of show is that implementing these ideas using something that\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"something that we've developed recently called LandGraph is a really nice approach and it works really well of local LMS that are much smaller, for example, than API-gated, very large scale foundation models. And so we're going to look at one\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"to look at one particular paper called Corrective Rag or Sea Rag. Now this paper is kind of, there's been some attention on, for example, Twitter about this work. It's a really neat paper. And the idea is actually pretty simple and straightforward.\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"straightforward. If you go down to the figure here, you're going to do, perform a treeval and you're going to grade the documents relative to the question. So you're kind of doing a relevance grading. And there's some heuristics, like basically if\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content='like basically if the documents are deemed correct, they actually do some knowledge refinement where they further strip the documents to compress relevant chunks within the documents and retain them. And if the documents are either deemed ambiguous', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"deemed ambiguous relative to the query or incorrect, it performs a web search and supplements retrieval with a web search. So that's kind of a big idea. But it's a nice illustration of this general principle of don't just do rag as kind of like a\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"as kind of like a single shot process for your performer treeval and then go to generation. You can actually perform self-reflection or reasoning. You can retry. You can retrieve from alternative sources and so forth. That's kind of a big idea. Now\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"of a big idea. Now in our build here, we're going to make some minor simplifications. Here's kind of a layout of the graph that we're interested in. We're going to perform a treeval and for that we're going to use no-mic embeddings which run\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"which run locally. We're going to build a node for grading. Those documents relative to the question to say are they relevant or not. And if any documents are deemed irrelevant, we'll go ahead and do a query rewrite web search, embles, then go ahead\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"then go ahead to generation based on the web search result. So that's the flow. Now first things first is how do I get started running LMS locally and kind of where do I go. And where I often direct people and what I found to be really useful is\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"be really useful is Olamah. It is a really nice way to run models locally, for example on your Mac laptop very easily and they are launching support for various other platforms as well. And so basically if you go to their website, it's very simple.\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"it's very simple. You simply download their application. You can see it's running here on my machine. And once you have it downloaded, all you need to do is you can go to their model list and you can kind of search around. So you can actually look,\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"can actually look, I think it's sorted by popularity. You can see Mistral, obviously a really interesting open source model is kind of one of the top. So you can see it has like 210,000 polls. It's one of the top models. I click on this. And where\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"on this. And where this takes me is a model page. I can look at this tags tab. And this basically shows me a bunch of model versions that I can really easily just like download and run. And we'll show how to do that here very shortly. What I'm going\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"What I'm going to do is I'm going to choose Mistral and struct. So that is there's 7 billion parameter and struct model. And so all I would do, I'm going to go over to my notebook here. So I have an empty notebook. And all I've done is I've already\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"is I've already done a few pip installs. And I've also set a few environment variables to use Langsmith. And we'll see why that's useful later. That's really all I've done. Now I'm going to put a note here to, for Olamma. And what I'm going to do is\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"I'm going to do is this Olamma pull the model I want and use run that. So normally this will take a little bit because you're actually pulling the model. And typically it's like a couple gigs. I actually already have this model. So it's faster. It's\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"it's faster. It's actually already done. But that's really all you do. Okay, so that's kind of like step one. And then what we're going to do is I'm just going to create this variable local LLM. That I am going to yeah, so I'm just going to define\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"going to define this variable. Mistral and struct because this is the model that I download using Alama pull Mistral struct, that's all that's going on here. So this is going to be the LLM I'm going to work with. I've pulled this so it's local on my\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"so it's local on my system. It's available via Olamma, which is basically running the background on my system. And you can see it's really seamless and easy to use. Now the first thing I want to do for this approach is I'm going to call this index.\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"to call this index. So because this was a corrective rag approach, I need an index that I care about that I'm actually performing rag on. And so here I'm going to use a particular blog post that I like on agents. We can like pull it up here and have\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"it up here and have a look. Let's pull it up over here actually. So this is a pretty neat blog post on autonomous agents. It's like pretty long and meaty. So it's kind of like a good target for performing retrieval on. It's a details here, really\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"here, really neat, really detailed blog post. So what I'm going to do is I'm going to load it here. I'm going to split it and I'm going to use a chunk size of 500 tokens. These are kind of somewhat arbitrary parameters. You can play with these as\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"play with these as you want. The point is here I'm just building up a quick local index. So I load it. I split it into chunks. Now this is the interesting bit. I'm going to use GPT for all embeddings from NOMIC, which is, let's actually pull up the\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content='pull up the link here. I had it available here. So these are, you can see right here, it is a CPU optimized contractively trained as basically SBIRT model. So you can like join the sentence transformers so you can see. Yep. So this is the initial', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"this is the initial work describing our paper, SBIRT basically. So the key point is this. This is a locally running CPU optimized embedding model that works quite well. I found runs on your system, no API, nothing. So it's pretty nice. Runs fast. So\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"nice. Runs fast. So we're going to go ahead and use that from our friends at NOMIC. And I'm also going to use Chrome, which is an open source local vector store that's really easy to spin up runs locally. And all I'm doing is I'm taking my\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"is I'm taking my documents, I'm going to define a new collection, taking my embedding model, GPT for all embeddings, I'm going to create a retriever from this. So there we go. Okay, it shows you some parameters. So cool, I have a retriever. So we\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"a retriever. So we can actually call, we can say get relevant documents. And I can say something about like, let's say like agent memory or something. You know, let's just test. And okay, cool, look at that. So it's nice and quick. We get a bunch of\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"We get a bunch of documents out that relate to memory. So you can see memory stream, like the documents are are sane. So it looks like everything's kind of working here. So that's great. We have a retriever. Now let's think a little bit about what\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"bit about what we want to do next. So when I do these kinds of kind of logical rag flows, but as graphs, first I always try to lay out the logic and move this up here. I try to lay out kind of the logical steps. And in each logical step, what's\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"step, what's happening is I'm transforming a state. So in these graphs, really all you're doing is you're defining a state that you're just modifying throughout the flow of the graph. Now in this case, because we're interested in rag, our state is\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"rag, our state is just going to be a dictionary. And that dictionary you can see, I actually kind of schematically laid it out here. It's just going to contain a few keys that are things relevant to rag. It's going to be like a question. Then it's\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"question. Then it's going to be you append documents to your dict and then eventually you're going to append generation. So that's really all that's going on in terms of like how your state's being propagated through the graph. And at every node,\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"And at every node, you're making some modification to state. That's the key point. So you're basically going to do you start with a question from the user, you perform retrieval relevant to the question, you're they're going to grade the documents.\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"the documents. So you're going to do a modification of the documents. Then you're going to make a decision, are they relevant or not? If they're not relevant, you're going to transform the query, modify the question, do a web search. The final step\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"The final step is a generation based on the tree of documents. So that's your flow. Now what I want to call out here is there's one very important what we call conditional edge where depending upon the results of the grading step, I want to do one\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"I want to do one thing or another. So I'm going to make a decision. So I want to show you something that's very convenient that we can use with Olamma to help us here. So this is, I'm going to make a note here to note what I'm going to highlight. So\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"to highlight. So this is Olamma JSON mode. So the basic logic of that conditional edge decided generate is going to be something like this. I already have this prompt laid out. But it's basically going to be, I'm going to take a document and I'm\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"a document and I'm going to take my question. And I'm going to do some kind of comparison to say is the document relevant to the question. That's really what I want to do. But here's the catch. Because I want that edge to process a very particular\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"a very particular output, either yes or no, I want to make sure that my output is structured in a way that can reliably be interpreted downstream in my graph. This is where JSON mode from Olamma is really useful. And you can see all I do is now I'm\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"all I do is now I'm importing shadow Lama. This is going to reference that local model that I specified up here, Mr. Onstruct, which I've downloaded. So I have the model locally. And I'm just flagging this format JSON to tell the model to output\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"the model to output JSON specifically. And what I'm going to do in my prompt here, I'm basically saying, you know, you're a greater. Here's the documents, here's the question. And here's the catch. Have a binary score yes, no, and provide it as JSON\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content='provide it as JSON with a single key score and no preamble no explanation. So I kind of explain in the prompt what I want. And when I call this with JSON mode, it will enforce that JSON is returned. And hopefully with a single key, we expect score', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"we expect score and either binary yes, no. And when I'm going to run that as a chain, so I'm going to supply that prompt to my L, and I'm going to then parse that JSON string out into a JSON object, which then I can work with. So let's try that.\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"So let's try that. We're going to try to run this chain we defined. We're going to run retrieval on here's a question. Here's our docs. Let's grade one of the docs using basically passing question and one document. And we're going to take the page\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"to take the page content from the document, which is like basically all the text. And we're going to run this. So let's test that quickly. And it is still running. Now it's finished. Let's check the output here. And we can see so we get a JSON back,\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "text_splitter.split_documents(text_documents)[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 1536\n",
      "[0.0003943086566131897, -0.03709936963004439, -0.010911204144300022, 0.0011529632847324287, -0.023190303249241876, 0.014407647137365998, -0.01196588972874877, -0.0005864692040685945, -0.01259870107941802, -0.03684368805046346]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedded_query = embeddings.embed_query(\"Who is Mary's Sister?\")\n",
    "\n",
    "print(f\"Embedding length: {len(embedded_query)}\")\n",
    "\n",
    "print(embedded_query[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajeevsingla/Developer/rag-ai/.venv/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec, PodSpec\n",
    "import time\n",
    "import os\n",
    "\n",
    "use_serverless = True\n",
    "pinecone_api_key = os.environ.get('PINECONE_API_KEY')\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "if use_serverless:\n",
    "    spec = ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "else:\n",
    "    # if not using a starter index, you should specify a pod_type too\n",
    "    spec = PodSpec()\n",
    "\n",
    "# check for and delete index if already exists\n",
    "index_name = 'langchain-retrieval-augmentation-fast'\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# create a new index\n",
    "pc.create_index(\n",
    "    index_name,\n",
    "    dimension=1536,  # dimensionality of text-embedding-ada-002\n",
    "    metric='dotproduct',\n",
    "    spec=spec\n",
    ")\n",
    "\n",
    "# wait for index to be initialized\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Hi, this is Lance from the chain team. I'm going to talk about building a self-reflective rag apps from scratch using only open source and local models that run strictly on my laptop. Now one of those interesting trends in the rag research and a lot\" metadata={'source': 'transcription.txt'}\n",
      "page_content='research and a lot of methods that become pretty popular in recent months and weeks is this idea of self-reflection. So when you do rag, you perform retrieval based upon a question from an index. And this idea of self-reflection is saying based' metadata={'source': 'transcription.txt'}\n",
      "page_content='is saying based upon, for example, the relevance of the tree documents to my question or based upon the generations relative to my question or the generations relative to the documents, I want to make, I want to perform some kind of reasoning and' metadata={'source': 'transcription.txt'}\n",
      "page_content=\"of reasoning and potentially feed back and retry various steps. So that's kind of the big idea. And there's a few really interesting papers that implement this. And what I want to kind of show is that implementing these ideas using something that\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"something that we've developed recently called LandGraph is a really nice approach and it works really well of local LMS that are much smaller, for example, than API-gated, very large scale foundation models. And so we're going to look at one\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"to look at one particular paper called Corrective Rag or Sea Rag. Now this paper is kind of, there's been some attention on, for example, Twitter about this work. It's a really neat paper. And the idea is actually pretty simple and straightforward.\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"straightforward. If you go down to the figure here, you're going to do, perform a treeval and you're going to grade the documents relative to the question. So you're kind of doing a relevance grading. And there's some heuristics, like basically if\" metadata={'source': 'transcription.txt'}\n",
      "page_content='like basically if the documents are deemed correct, they actually do some knowledge refinement where they further strip the documents to compress relevant chunks within the documents and retain them. And if the documents are either deemed ambiguous' metadata={'source': 'transcription.txt'}\n",
      "page_content=\"deemed ambiguous relative to the query or incorrect, it performs a web search and supplements retrieval with a web search. So that's kind of a big idea. But it's a nice illustration of this general principle of don't just do rag as kind of like a\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"as kind of like a single shot process for your performer treeval and then go to generation. You can actually perform self-reflection or reasoning. You can retry. You can retrieve from alternative sources and so forth. That's kind of a big idea. Now\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"of a big idea. Now in our build here, we're going to make some minor simplifications. Here's kind of a layout of the graph that we're interested in. We're going to perform a treeval and for that we're going to use no-mic embeddings which run\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"which run locally. We're going to build a node for grading. Those documents relative to the question to say are they relevant or not. And if any documents are deemed irrelevant, we'll go ahead and do a query rewrite web search, embles, then go ahead\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"then go ahead to generation based on the web search result. So that's the flow. Now first things first is how do I get started running LMS locally and kind of where do I go. And where I often direct people and what I found to be really useful is\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"be really useful is Olamah. It is a really nice way to run models locally, for example on your Mac laptop very easily and they are launching support for various other platforms as well. And so basically if you go to their website, it's very simple.\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"it's very simple. You simply download their application. You can see it's running here on my machine. And once you have it downloaded, all you need to do is you can go to their model list and you can kind of search around. So you can actually look,\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"can actually look, I think it's sorted by popularity. You can see Mistral, obviously a really interesting open source model is kind of one of the top. So you can see it has like 210,000 polls. It's one of the top models. I click on this. And where\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"on this. And where this takes me is a model page. I can look at this tags tab. And this basically shows me a bunch of model versions that I can really easily just like download and run. And we'll show how to do that here very shortly. What I'm going\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"What I'm going to do is I'm going to choose Mistral and struct. So that is there's 7 billion parameter and struct model. And so all I would do, I'm going to go over to my notebook here. So I have an empty notebook. And all I've done is I've already\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"is I've already done a few pip installs. And I've also set a few environment variables to use Langsmith. And we'll see why that's useful later. That's really all I've done. Now I'm going to put a note here to, for Olamma. And what I'm going to do is\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"I'm going to do is this Olamma pull the model I want and use run that. So normally this will take a little bit because you're actually pulling the model. And typically it's like a couple gigs. I actually already have this model. So it's faster. It's\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"it's faster. It's actually already done. But that's really all you do. Okay, so that's kind of like step one. And then what we're going to do is I'm just going to create this variable local LLM. That I am going to yeah, so I'm just going to define\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"going to define this variable. Mistral and struct because this is the model that I download using Alama pull Mistral struct, that's all that's going on here. So this is going to be the LLM I'm going to work with. I've pulled this so it's local on my\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"so it's local on my system. It's available via Olamma, which is basically running the background on my system. And you can see it's really seamless and easy to use. Now the first thing I want to do for this approach is I'm going to call this index.\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"to call this index. So because this was a corrective rag approach, I need an index that I care about that I'm actually performing rag on. And so here I'm going to use a particular blog post that I like on agents. We can like pull it up here and have\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"it up here and have a look. Let's pull it up over here actually. So this is a pretty neat blog post on autonomous agents. It's like pretty long and meaty. So it's kind of like a good target for performing retrieval on. It's a details here, really\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"here, really neat, really detailed blog post. So what I'm going to do is I'm going to load it here. I'm going to split it and I'm going to use a chunk size of 500 tokens. These are kind of somewhat arbitrary parameters. You can play with these as\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"play with these as you want. The point is here I'm just building up a quick local index. So I load it. I split it into chunks. Now this is the interesting bit. I'm going to use GPT for all embeddings from NOMIC, which is, let's actually pull up the\" metadata={'source': 'transcription.txt'}\n",
      "page_content='pull up the link here. I had it available here. So these are, you can see right here, it is a CPU optimized contractively trained as basically SBIRT model. So you can like join the sentence transformers so you can see. Yep. So this is the initial' metadata={'source': 'transcription.txt'}\n",
      "page_content=\"this is the initial work describing our paper, SBIRT basically. So the key point is this. This is a locally running CPU optimized embedding model that works quite well. I found runs on your system, no API, nothing. So it's pretty nice. Runs fast. So\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"nice. Runs fast. So we're going to go ahead and use that from our friends at NOMIC. And I'm also going to use Chrome, which is an open source local vector store that's really easy to spin up runs locally. And all I'm doing is I'm taking my\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"is I'm taking my documents, I'm going to define a new collection, taking my embedding model, GPT for all embeddings, I'm going to create a retriever from this. So there we go. Okay, it shows you some parameters. So cool, I have a retriever. So we\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"a retriever. So we can actually call, we can say get relevant documents. And I can say something about like, let's say like agent memory or something. You know, let's just test. And okay, cool, look at that. So it's nice and quick. We get a bunch of\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"We get a bunch of documents out that relate to memory. So you can see memory stream, like the documents are are sane. So it looks like everything's kind of working here. So that's great. We have a retriever. Now let's think a little bit about what\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"bit about what we want to do next. So when I do these kinds of kind of logical rag flows, but as graphs, first I always try to lay out the logic and move this up here. I try to lay out kind of the logical steps. And in each logical step, what's\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"step, what's happening is I'm transforming a state. So in these graphs, really all you're doing is you're defining a state that you're just modifying throughout the flow of the graph. Now in this case, because we're interested in rag, our state is\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"rag, our state is just going to be a dictionary. And that dictionary you can see, I actually kind of schematically laid it out here. It's just going to contain a few keys that are things relevant to rag. It's going to be like a question. Then it's\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"question. Then it's going to be you append documents to your dict and then eventually you're going to append generation. So that's really all that's going on in terms of like how your state's being propagated through the graph. And at every node,\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"And at every node, you're making some modification to state. That's the key point. So you're basically going to do you start with a question from the user, you perform retrieval relevant to the question, you're they're going to grade the documents.\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"the documents. So you're going to do a modification of the documents. Then you're going to make a decision, are they relevant or not? If they're not relevant, you're going to transform the query, modify the question, do a web search. The final step\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"The final step is a generation based on the tree of documents. So that's your flow. Now what I want to call out here is there's one very important what we call conditional edge where depending upon the results of the grading step, I want to do one\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"I want to do one thing or another. So I'm going to make a decision. So I want to show you something that's very convenient that we can use with Olamma to help us here. So this is, I'm going to make a note here to note what I'm going to highlight. So\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"to highlight. So this is Olamma JSON mode. So the basic logic of that conditional edge decided generate is going to be something like this. I already have this prompt laid out. But it's basically going to be, I'm going to take a document and I'm\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"a document and I'm going to take my question. And I'm going to do some kind of comparison to say is the document relevant to the question. That's really what I want to do. But here's the catch. Because I want that edge to process a very particular\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"a very particular output, either yes or no, I want to make sure that my output is structured in a way that can reliably be interpreted downstream in my graph. This is where JSON mode from Olamma is really useful. And you can see all I do is now I'm\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"all I do is now I'm importing shadow Lama. This is going to reference that local model that I specified up here, Mr. Onstruct, which I've downloaded. So I have the model locally. And I'm just flagging this format JSON to tell the model to output\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"the model to output JSON specifically. And what I'm going to do in my prompt here, I'm basically saying, you know, you're a greater. Here's the documents, here's the question. And here's the catch. Have a binary score yes, no, and provide it as JSON\" metadata={'source': 'transcription.txt'}\n",
      "page_content='provide it as JSON with a single key score and no preamble no explanation. So I kind of explain in the prompt what I want. And when I call this with JSON mode, it will enforce that JSON is returned. And hopefully with a single key, we expect score' metadata={'source': 'transcription.txt'}\n",
      "page_content=\"we expect score and either binary yes, no. And when I'm going to run that as a chain, so I'm going to supply that prompt to my L, and I'm going to then parse that JSON string out into a JSON object, which then I can work with. So let's try that.\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"So let's try that. We're going to try to run this chain we defined. We're going to run retrieval on here's a question. Here's our docs. Let's grade one of the docs using basically passing question and one document. And we're going to take the page\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"to take the page content from the document, which is like basically all the text. And we're going to run this. So let's test that quickly. And it is still running. Now it's finished. Let's check the output here. And we can see so we get a JSON back,\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"we get a JSON back, which is just the score. Yes, no. So that's exactly right. That's what we want. And we can actually look under the hood here at, yeah. So we can actually look under the hood in Langsmith at the grading process. And we can see\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"And we can see here that our prompt got populated with the context. So here is a document. And right, here is the question. Here is a document. And the task was of course to grade it. So we can see here's like the full prompt. You're greater\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"You're greater assessing the raw blood surgery document. Here's a document. And then here's the model output score. Yes. So this is really nice. We basically enforce the output from our local LLM using JSON mode. So we know every time it's going to\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"time it's going to output binary yes, no score as a JSON object, which we think is an extra. So that's a very key point that I just wanted to flag. It's a very nice thing that Alama offers that's extremely helpful when building out, particularly\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"out, particularly these kind of logical graphs where you really want to constrain the flow at certain edges. So that's kind of the like really key thing I wanted to highlight. A lot of the rest of this is actually pretty straightforward. So let's\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"So let's now define our graph state. This is the dictionary that we're going to basically pass between our nodes. So this is just some code I'm going to copy over. This is defining your graph state. You're just saying it's a dick. That's all there\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"That's all there is really to that. Now here is where I'm going to copy over some code that basically implements a function for every node and every conditional edge in our graph. So if you remember, we can kind of go over and look, our graph is\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"look, our graph is laid out like this. And all we're doing is for every node drawn, we're going to find a corresponding function here, which performs some operation. So retrieve is basically just doing, we had our tree redefined, get relevant\" metadata={'source': 'transcription.txt'}\n",
      "page_content='get relevant documents, and write them out to state. So again, we take a question in. So if you look here, we basically have this state dick passed into the function. We extract the state dick here. We extract the question from the state dick. We do' metadata={'source': 'transcription.txt'}\n",
      "page_content=\"state dick. We do retrieval and we write that state dick back out to the output. So you think about every node is just doing some modification on the state. Doing it in, doing something, writing it back out. That's really all that's going on. And we\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"going on. And we can really just march across our little diagram here and see how basically each one of these nodes is implemented as a function. And again, you can see in every case, we're using, for example, Chattolama, in some of these cases, we\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"of these cases, we don't need JSON mode. So if we're just doing like a generation step, as you can see here, we don't need JSON mode for the grading we do. So we're actually going to implement here the same thing we just showed. Chattolama using\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"Chattolama using JSON mode. And what's going to happen is we can see we generate our score every time. And then we can extract our grade from that JSON. And then we know the grade is going to constrain to the output yes or no. Then here's the key\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"Then here's the key point. We do some logical reasoning on that to say, for example, if the grade is yes, then we're going to like append the document, it's relevant. If not, then what we're going to do is we're going to filter that document out.\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"that document out. And we're also going to set this flag to search, perform web search as yes. So what really happening here is we are kind of applying kind of a logical gate to say, if any document is scored as relevant, then we just add it to our\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"just add it to our final list of filter documents. If not, we're going to go ahead and do a web search. And we're going to set the search flag to be yes. And we're not going to include that document in the output. And you can see here, we return a\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"here, we return a dictionary, which contains our filter documents, our question, and then that flag to run web search yes or no. You can see it was default no. But if we ever encounter a new relevant document, we change that to yes. So that's really\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"So that's really all that's going on here. You can see we do our query transform down here again. We just use mistrol again. Here is like a transform prompt. But you kind of get the idea. Web search no, we use tably web search here. It's really kind\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"It's really kind of a nice quick way to perform web searches. And you can see we just supplement the documents with the web search results. And then this was kind of the final step where we wrote out yes or no to our search key. And depending upon\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"And depending upon the state, which we can read in here, we make decision to basically either return transform query or return generate, which will basically, that's determining the next node to go to. So this decide to generate is our conditional\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"is our conditional edge. That's actually right here. And so it's looking at the results that we wrote out from great documents in particular that search yes or no key in our dict. And it's then going to basically determine the next node to diverse\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"node to diverse to that's really all we're doing here. So that's kind of nice. Now what we're going to do is we kind of copied over all these functions. We then can go ahead and run that. And now we just lay out our graph. So again, our graph was\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"our graph was kind of explained here. And here's where we actually just lay out the full kind of graph organization. How we're going to connect each node. So we add the nodes first. We set our entry point. And then we add the edges accordingly\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"edges accordingly between the nodes. And basically the logic here just maps over to our diagram here. That's really all that's happening. Cool. So I'm going to go ahead and go down. And now let's kind of see this all working together. So I'm going\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"So I'm going to go ahead and compile my graph. I'm going to go ahead and ask a question, explain how the different types of agent memory work. And what I'm going to do, let's go back to our diagrams, we can kind of reference that. I'm going to call\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"I'm going to call this. And I'm actually just going to like this will like traverse every step along the way. And it'll print out something to explain what's happening. So you can see a performer tree bowl and now do migrating steps. And this is all\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"And this is all running locally. And they were all deemed relevant. So then I'm going to go ahead and generate. And it's running right now. And there we go. So we can go over to Lang Smith. And let's actually have a look at what happened under the\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"happened under the hood. Since this is what just ran. So we can see that at each one of these steps we call chat alama with our mistral 7B model that's running locally. And this is our grading step. So this was each document being graded. So again,\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"graded. So again, like look at this. So it outputs a binary score yes, no, as a dict. That's great. So this has a bunch more down here. So these are all of our documents graded. And now here is that final alama call, which basically packed that all\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"packed that all into our rag prompt. You're an assistant for question answering tasks. He's a following to answer the question. Here's all of our docs. Here's the answer. So that's pretty cool. We can see that this multi-step logical flow all works.\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"flow all works. Now let's try something kind of interesting. I'm going to ask a question that I know is not in the context and see if it will kind perform that default to do web search. So I'm going to say explain how alpha codium works. So this is\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"works. So this is a recent paper that came out that's not relevant at all to this blog post. So I know that retrieval should not be considered relevant. And let's go ahead and run that and convince ourselves that that's true. So good. This is\" metadata={'source': 'transcription.txt'}\n",
      "page_content='So good. This is perfect. So the greater is determining these documents are not relevant. And so it should be making the decision to perform web search. So it should be kind of going to this lower branch transform the query run web search. And it' metadata={'source': 'transcription.txt'}\n",
      "page_content=\"web search. And it looks like that all ran. So it tells us alpha codium is double source AI coding generation tool developed by code M and I. This is perfect. That's exactly what it is. And we can actually go into Langsmith and again see what\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"and again see what happened here. So you can see here the trace is a little bit more extensive because all of our grades are incorrect. It's now irrelevant. Again, we get the nice JSON out. And okay. So this is pretty cool. So this was our question\" metadata={'source': 'transcription.txt'}\n",
      "page_content='was our question rewriting node. So basically provide an improved input question without any preamble. So what is the mechanism behind alpha codium functionally? So it modifies a question. We use to alley search right here. So it basically does' metadata={'source': 'transcription.txt'}\n",
      "page_content=\"it basically does retrieval. It searches for stuff related to alpha codium. So that's great. And then we finally passed that to our model for generation based on this new context. And there we go. So the code is almost for say I code assistant tool.\" metadata={'source': 'transcription.txt'}\n",
      "page_content='assistant tool. So that kind of gives you the main idea. And the key point is this is all running locally. Again, I used GPT for all embeddings for indexing up at the top right here. And I used Olawa with mistral 7B instruct and JSON mode for that' metadata={'source': 'transcription.txt'}\n",
      "page_content='JSON mode for that one crucial step where I need to constrain the output to be kind of a score of yes, no. And for other things, I just use the model without JSON mode to do perform generations, like to question rewrite or to do the final' metadata={'source': 'transcription.txt'}\n",
      "page_content=\"or to do the final generation. So in any case, I hope this gives you kind of an overview of how to think about building logical flows. Doesn't have to be ragged. But rag is a really good kind of use case for this using local models and land graph.\" metadata={'source': 'transcription.txt'}\n",
      "page_content='and land graph. And the thing I want to kind of leave you with is there is a lot of interest in complex logical reasoning using local elements and a lot of focus on using agents. And I do want to kind of encourage you to think about depending on the' metadata={'source': 'transcription.txt'}\n",
      "page_content=\"depending on the problem you're trying to solve, you may or may not actually need an agent. It's possible that kind of implementing a state machine or a graph kind of as shown here with some series of logical steps. This can incorporate cycles or\" metadata={'source': 'transcription.txt'}\n",
      "page_content=\"cycles or loops back to like prior stages. We have some more complex examples that show that. This actually can work really well with local models because the local model is only performing a step within each node. So you're kind of constraining it\" metadata={'source': 'transcription.txt'}\n",
      "page_content='of constraining it to like just do this little thing, just do this little thing, like just rewrite the question, just grade the document rather than using the local LM as like an agent executor that has to make all these decisions kind of jointly or' metadata={'source': 'transcription.txt'}\n",
      "page_content='kind of jointly or kind of in a less controlled workflow. Where for example, like the ordering of these various tasks can be determined arbitrarily by the agent. Here we really nicely constrain the logical flow and let the local model just do little' metadata={'source': 'transcription.txt'}\n",
      "page_content=\"just do little tasks at each step. And I've just found it to be a lot more reliable and really useful for these kinds of like logical reasoning tasks. So hopefully this is helpful, give it a try and we'll make sure all this code is easily shared.\" metadata={'source': 'transcription.txt'}\n",
      "page_content='is easily shared. Thank you.' metadata={'source': 'transcription.txt'}\n"
     ]
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()\n",
    "len(text_splitter.split_documents(text_documents))\n",
    "for batch in text_splitter.split_documents(text_documents):\n",
    "    print(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "documents = text_splitter.split_documents(text_documents)\n",
    "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index_name = \"langchain-retrieval-augmentation-fast\"\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents, embeddings, index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='assistant tool. So that kind of gives you the main idea. And the key point is this is all running locally. Again, I used GPT for all embeddings for indexing up at the top right here. And I used Olawa with mistral 7B instruct and JSON mode for that', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"play with these as you want. The point is here I'm just building up a quick local index. So I load it. I split it into chunks. Now this is the interesting bit. I'm going to use GPT for all embeddings from NOMIC, which is, let's actually pull up the\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"this is the initial work describing our paper, SBIRT basically. So the key point is this. This is a locally running CPU optimized embedding model that works quite well. I found runs on your system, no API, nothing. So it's pretty nice. Runs fast. So\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search(\"What is GPT-4\")[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The content includes a blog post on autonomous agents, a discussion on a paper called Corrective Rag or Sea Rag, and the use of a rag prompt for question answering tasks.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"Details on the content?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
