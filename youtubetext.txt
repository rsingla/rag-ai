Hey, so today I'm going to show you how to run a local LLM on your computer and how to build an entire rack system, a retrieval augmented generation system, using those locally running LLMs. And these are going to be open source models that we're going to use. Here is the reason why this is crucial, right? So even though the GPT models are so far the best at solving most problems that we want them for, knowing how to use a local LLM is very important. First, because these open source models are getting really, really good. Number two, because they're cheaper and you don't need all of the power from GPT to solve certain use cases. So if you know how to use a local model, now you can do the same task at the same level of quality for much cheaper. Number three, for privacy reasons. Many, many companies do not want to use the GPT models. They don't want to connect to an external API. They want to have everything in house and that's what an open source model will give you. Also, if you are planning to deploy one of these models into a scenario where you don't Let's say robotics, right? Or an edge device. There is no chance for you to just connect to an API. You will have to use an open source model. So those are some of the reasons. There is one that's also my favorite one that is even if you're using open AI as your primary model, you can use one of these open source models as a backup. So if the open AI API goes down, the other day they had downtime for a day. I think the model was just returning nonsense. You can immediately flip to an open source model and keep your workflow running with no disruption. So many, many reasons. The goal of today's video is to show you how to do that on your computer starting from nothing. So I'm going to start with an open browser. I'm going to do that. I'm going to make it run. I'm going to build that simple, very simple rack system. We're going to read or answer questions from a PDF. So I'm going to get a website, download it as a PDF and answer questions from that. The most important thing is that here, the thing that matters the least is the code that I'm going to write. That is not what's important about this. Anyone can write the code. The thing that matters the most is the reasoning behind that code. Why do we need to do this or that? That is what I want to convey on this video. And I hope that's what you get out of it, right? The understanding of the stuff that we're building here and the reasoning behind it. All right. So we know, finishing that introduction, let me get here to my browser. And I opened, we're going to start here, very simple. I opened this website, it's Olama. So Olama is the project that is going to allow us to run an open source model in our computer. Okay. So here is the thing. Here is the thing that I want you to know. Like, when you think about a model, I want you to think about a gigantic mathematical formula because that's what it boils down to. It's just a bunch of values, weights and biases, we call them parameters, and they're put together in a gigantic mathematical formula. That is huge. When we talk about Lama2, the 7b model, that 7b means seven billion parameters. So it refers to the number of weights and biases that we need to store and execute in order to make any prediction. Lama270b, that's 70 billion parameters. So when we download one of these models, what we are downloading is all of those values, all of those parameters, the 70 billion parameters, we're storing that in our hard drive. And we're storing some sort of like instructions on how to put together those parameters and run them as a big mathematical formula. That's basically what we're storing. So Olamma is going to serve as the common wrapper around all of these different models. So we can download Lama2, but we can also download Mixed Troll, and we can run them through Olamma, through this common interface. So to install Olamma is very simple. Go to this website, olamma.com. They have versions for Mac, Linux, and they just release a preview for Windows. So even if you're on Windows, you can run Olamma on your computer. So I already downloaded it. It's just it's very thin. It's just it's very small. When you download it, you can run it. And you can see here on my status bar, you can see there is a little Lama right there. That tells me that Olamma is running on my computer. By the way, the first time you run Olamma, it's going to ask you to install the command line tools. I think that's what it does or to download the Lama2 model. I don't remember what is the first instruction they give you, but it's very simple to navigate and it's very quick. So there is here in the website, you're going to see a link. It's called models. So click on that. And here is the list of all of the models that you can run using Olamma. OK. So here you have the Genma models that just Google released. I think it was a week ago. This was updated two days ago. Jesus, it is so fast. You get Lama2, Mistral, Mixedrol with an X. Lava, I mean, it just goes on and on and on. If you care about code, you have code Lama here. So all of these models you can download. Now, how did you do that? I mean, you can obviously you can go here inside one of these models and see the entire family. You see the 7B version, the 70B version, the chat model. You can you know, you can explore this on your own. You're going to get information about your your model, how to use it, etc. This is what I'm going to do. It's going to be very, very simple. So I'm going to go to my command line and you're going to have Olamma as the as a command after you install it. And here you can see a bunch of available commands here. You get the pull command. So basically you can say Olamma pull Lama2 and that will download the latest version of Lama2 to your computer. OK, I already did that. I'm not going to do it here. Plus it takes I mean, it's downloading. I think it's like 20 gigabytes of data. So it's going to take a few a few minutes to do. You can do Olamma list and that will tell you what are the models that I have installed here on my computer. I have the latest version of Lama2. I also have mixed roll the A times 7B version and I have the latest version of mixed roll. OK, and you can see here the ID. You can see the size 26 gigabytes mixed roll Lama2 is only three gigabytes when it was modified. I have here just to show you when you download a model, that is going to get I don't know how big this is on the recording. But when you download a model, it basically downloads these files within a folder. I'm on a Mac here, so it will download within my base directory. There is a dot Olamma directory and it will put in there every file that I download. And you can see like some of these like this one here is the mixed roll one. See the 26 gigabytes that is just basically all of the parameters are stored there after the latest training version of mixed roll. So all of those values are going to be stored here. So just so you know, if you want to delete them, where to come. You can also obviously delete them using the RM command here. So now that I have these, I think I can do I don't remember the command, but I'm going to do Olamma. Lama two is not like that. So how do I serve this? Oh, there we go. Maybe serve. Let's see. Show run. Ah, let's do run. There we go. So now we're running Lama two here on my computer. Okay. And now I can say, tell me a joke. And there we go. Sure. Here is one. Why don't scientists trust that this is just a bad joke? I'm sorry. So anyway, here is the model running on my computer, which is awesome. If I do this, this is going to give me the help. Let's just say bye. Okay, awesome. So now I have an open source model running on my computer from the command line. That's great. But that's not what I want. What I want is to be able to access this model programmatically. So in order to do that, I'm going to create a directory here and we're going to build a very simple rack system from scratch using LAN chain to get data from a PDF file. Okay. So that's what I want to do. So I'm going to create a directory. I'm going to call it, I don't know, local model. Let's call it local model. Okay. And let's go open Visual Studio code on that directory. There we go. Local model. Okay, awesome. So here is Visual Studio code. I'm going to make it nice and big and beautiful. You guys can see. And I'm going to create a notebook. I'm going to call it notebook. This is a Jupyter notebook. Just so you know, in order for you to be able to use Jupyter within Visual Studio code, you need to install the Jupyter plugin. This plugin is created by Microsoft and obviously the Python plugin because this is going to be Python. But yeah, I'm going to be using Jupyter from within my notebook here. Awesome. I'm going to open a terminal window within Jupyter and I'm going to create a virtual environment so I can install all of the libraries that we're going to need within that virtual environment. I don't want to be installing anything directly on my computer. So from within Jupyter notebook here from within the terminal, I'm going to do Python 3 to just call a module. And the module is VM. That's the virtual environment module that comes with Python. And I'm going to call it .vmv. That's the name of the folder that it's going to create. So I'm going to do that and that is going to execute. And now I have a new folder. Here is a hidden folder, but it's going to be a new folder that's going to be called .vmv. Many people use different virtual environments. They use poetry and they use Conda, MiniConda. I'm an old school guy. I want to keep it very simple. That's why I'm using the virtual environment here. All right. So let me just go inside that virtual environment. Cool. And now I can start installing stuff. OK, so what do I need here? Well, I don't know still what I need, but let's first start by just doing something from this notebook, just making sure this notebook is actually working. It tells me, hey, you need to select a kernel. What is the kernel? I'm going to go to the Python environment that I just created. I'm going to select that. Visual Studio is going to tell me that it's going to install everything that it needs to execute that print line. And boom, it runs. So this is awesome. This is working. OK, so this is something that we are going to need here. I'm going to need an environment file. So this environment file is just going to store any environment variables that I'm going to use during this presentation. So for this presentation, we're going to need the OpenAI key. We're going to need to pass that. Obviously, I don't have the value here, but this is going to be something like blah. And I'm going to store it there. And then I want to read that key from my code, from my notebook so I can get access to the OpenAI key. Why do I need access to the OpenAI API? Because I want to test everything that I'm doing locally also with GPT just to make sure how they compare. So that's why I'm going to access the OpenAI key. So I have my key over here. I'm going to paste it. Maybe in order to do that, I don't want to just put it on the on the screen. So you guys don't just use my key to build your applications for free. OK, I know you guys are not seeing what I'm doing, but I'm basically just pasting my OpenAI key off screen. So I know I could do it here and then just change it. But anyway, so after doing that, this is the stuff that I'm going to do. So I'm importing the OS library and I'm importing this library. It's called dot emph. And that's going to read the environment variables that I just store in the data and file. It's going to read that in memory when I call this load dot emph. And now I can use my OpenAI key just like this. It's just very simple. Now, in order for me to be able to use this, I need to install. It's called Python dot emph. So I need to install that library. Boom. Awesome. And then I'm defining here three. It's the same variable, just overriding the shorter. I'm going to start by using the GPT three point five. I'm going to start by using this just a variable. Boom. That works. OK, so now I'm ready to just use in line chain, create a very simple model just to make sure that the API is working. OK, so I'm going to be copying the code here. Let's see. So how do we how do we access this? I'm going to import the chat that OpenAI model. Yeah, thanks to to copilot. I'm going to create a model chat that OpenAI. I'm going to pass the OpenAI key. I'm going to pass the definition of the model, which is going to be or the name of the model, which is going to be GPT three point five. And now that I have here, I can just do invoke and I can pass. Hey, tell me a joke. OK, so I need to install line chain OpenAI, of course. I'm going to do that. What else I need to install more stuff? This might have been installed, but just in case I'm going to install the line chain as well. Yeah, it wasn't OK. Awesome. So now can I run this? Boom, obviously, this is beautiful. OK, so here we go. I asked the OpenAI API tell me a joke and I get back. Why couldn't the lower I get back a response from OpenAI? That is awesome. But this is just chat GPT or the GPT model. We don't want that. We want to do it from the locally. Running model. So how do we do that? It's very simple. We need to use the instead of using the specifically the OpenAI chat model, we need to use an Olamo model. And I think that's on a different library. Might have been. I'm not sure. I think it's in the line chain community library. Let's see. Let's import it here and we're going to get this class, which is Olamo. And what I want to do is I want to instantiate an Olamo model if. If the model is not a GPT model, so I'm going to do something like this. If model sorry, if model starts with not not like this, but the other way around GPT, then do. This right else. OK, let's do an Olamo model. By the way, I can take this out. And do it regardless. OK, so this is awesome. So basically, if I define that my model is a GPT, then I want to instantiate model using the chat OpenAI model. If not, I want to instantiate the model using the class Olamo passing the name of the model. In this case, I'm going to test this just using GPT. Let's see. So that should work like it did before. It does. Now I'm going to change the name of the model. Let's do Lama 2. So I'm going to change the name of the model. And there we go. You get an answer back from Lama. Notice something interesting here. Notice that when I call the Lama model, I get back a string. That's what I get back from the Lama 2 model. But when I call the GPT model, I get back a string. So when I call the GPT model, I get an AI message instance here that has the content inside. So the reason that's happening is because the GPT turbo model, this model here, is what we call a chat model. So it's a model that's meant for a conversation. Right. They're going to be AI messages, which is what I get here. They're also going to be human messages, which is like the questions that I'm asking about. So for example, they're going to be classified as human messages. So when I interact with that class, Lama chain would return a special structure. In this case, it's an AI message instance containing the content inside. So but the Lama 2 model is a completion model. It's not a chat model. I could be using a chat model as well, but I'm not. I'm using a completion model. And that's why what you get back is a string. So how do we fix this? Or just it's not a problem that we need to fix, but I don't like to see an AI message here. Well, we can use Lama chain to parse out that AI message. Just remove that and turn it into a string. Right. So Lama chain supports the concept of parsers. And I have I have here what we need to do. And you're going to see how simple it is. So let me paste it here. So I'm going to import a string output parser and a parser again. It's just a class that's going to take an input and it's going to transform it in one way. In this case, this one here is going to transform the input into a string, which is what we want. And I'm going to create my first Lama chain chain here. Lama chain is language chain. I'm going to create a chain here. That's where the name comes from. So my chain is going to take the model and I want to pipe the output of that model into the input of the next component. In this case, it's a parser. So if I do this, basically what's going to happen is that Lama chain will talk to the model, will send a request to the model, and then we'll get the output from the model and we'll pipe that output into the input of the parser, which then will return the string. So if I do this, I'm going to re-execute this line. I get my AI message, but I'm going to do it again. Just now from the chain. So I'm invoking the chain, not the model anymore. I'm invoking the chain. So the parser gets involved. When I do that, boom, I just get back the string. Why? Because this parser, that's the job. I'm going to remove the parser, invoke it now. You get the AI message. I put back the parser, invoke it now. You get the string. See how that works? That's beautiful. And that is one of the main characteristics of Lama chain. You can create increasingly more complex chains using different components. OK, but this is just the beginning. We can now run a model and that model can run locally. I have a Lama here and we know how to do a chain. Let me just build a simple rack system. Just very, very simple. I want to answer questions from a PDF. So the first question is what is going to be that PDF? So I'm going to go to this website here. And this is the class that I that I teach. I teach a live program. It's called the machine learning school program. Well, it's actually called building machine learning systems that don't suck. And I'm going to save this as a string. OK, so I'm going to go here. Like if I were to print this, I'm going to save my whole website as a PDF. So let me save that as a PDF and I'm going to save it the same folder. I'm going to put it in the same folder. Let's see, what do I put it here? This is just it's horrible. The dialogue that Apple decided to create to save components. I just hate it so much. All right. I'm going to call it Emil School, which is the name of the website. OK, so I'm going to call it Emil School. Boom. Let's go back here. Here we go. We have our PDF right there. So what I want to do now is use my model to answer questions from that PDF. The first thing that we need to do is load that PDF in memory. So how do we do that? We're going to need a library that I have it here somewhere. OK, the library is P install. It's called PI PDF or Python PDF, however you want to call it. So we need to install that library and then using land chain. We can actually load that in memory. So we're going to come here and look at this land chain supports document loaders. And by the way, there are a bunch of different document loaders that you can use to load information from anywhere. OK, so I'm going to use the PI PDF loader. That's why I needed the library in the first place. I'm going to type what the name of the PDF is going to be here. And I'm going to load it and split it. And that is very important. And then I'm printing out the pages so you see what the result was. I'm going to execute this and see what just happened. So land chain using this loader class, loaded and split it, my PDF into different pages. You're going to see here it says page one of 14, page two of 14, all the way to page 14 of 14. So I split my entire PDF document into different pages and loaded each one of those pages in memory. OK, so I have 15 pages or 14 pages in memory right now. That's awesome. That was a great, great step. I have that in memory here. The next thing is creating a template. So I need to create a template. And when I say a template, it's a prompt template to ask the model to answer questions from specific context. So let's do that. And let's do it the right way. So look at this, by the way, all of this stuff, all of the steps that I'm covering here talking about retrieval, augmented generation systems I covered in much more details in a different video. I'm going to put it somewhere here or maybe the description below. Or if not, you can find the latest video in my in my YouTube channel and you're going to see more detail about these steps here. So, OK, so here is a prompt template that we are going to use to pass to the model. So here is the stream. It says answer the question based on the context below. If you can answer the question, reply, I don't know. OK, you can make this this more complex if you want it. This is good enough for us. Then I'm going to provide the context and then I'm going to provide the question that I want to answer. I'm basically telling the model the question, please answer it using this information. Don't go to your memory. Don't go to the stuff that you've learned before. Just answer out of this section here. OK, so ideally, I'm going to be able to grab some of the pages on that PDF, put them here, the content of the pages, and then answer a question or ask a question about those pages. So I'm going to create this prompt template. Notice these two squiggly braces here, context and question, those are variables and Lanchain will turn them into variables that I can pass and provide values for. So here is my prompt. I'm using the prompt template, a class to create a template from the string that I just passed right here. And now I'm just just to test it out. I'm calling the format function. I'm saying, hey, format my prompt, passing context. See how context just became a variable here or an argument to this function. Here is some context on the question. Here is a question. So I'm going to execute these just to make sure this works. And you can see actually, let me do a print here. Let's see if those no lines much better, much better like this. Answer the question based on the context below. If you can answer the question, reply, I don't know. And then we say context. Here is some context. And then we say question. Here is a question. OK, so that's awesome. We have a prompt. So how do we pass this prompt into the model? Well, we can just keep building our chain. Remember that our chain was like this. And we were piping that into a parser so we could make this chain better if we do something like this. So what happens if we do this? We have a chain, we start with a prompt and that that prompt is going to go into the model and that that model is going to go into a parser. So we can create that. The question now is, what do you think is going when we say chain invoke? What do you think we need to pass? Well, remember, there are two variables that we need to provide. Therefore, we're going to have to invoke this chain, passing context and passing a question. So if I say the name I was given was Santiago. And then I'm going to ask, what's my name? Oops. And if I do this, your name is Santiago. Boom. That works. Important lesson here. Notice how this chain, when we invoke the chain, we need to understand what the input of the chain will be. Now, there is something that might help there. There is an input schema functionality that you can call the chain. I mean, obviously, you can just look at the first component of the chain. And if you understand what the first component is waiting for or is expecting, that is the invocation that is going to need to happen. But I find this input schema trick or tool very helpful because it tells me it gives me information about the chain without me having to overanalyze what that chain looks like. So in this case, you see, OK, so this is the chain. What is the input schema to that chain? And it talks about the prompt input. So it's going to be the prompt. And it tells me, well, the properties is expecting an object. And the properties of that object are a context, OK, which is a string and a question, which is also a string. So these are the two variables, context and question. See? So that is why I know that I need to invoke that chain with the context and the question. OK, awesome. So what do we have right now? We have a chain that already has a prompt. A model and a parser. We have the documents in memory, the documents, I mean, the PDF document. We already have it in memory, split by pages. Now we need to find a way to take that document and pass it as a context, but only pass the relevant portions of that document as a context. So how do we do that? I'm going to use a very simple vector store that is going to do several things for us. So number one, it's going to save. It's going to serve as a database for the content in a different way. We're not going to store the pages of the content just straight into the database. We are going to be storing embeddings of those documents. So we're going to get the whole PDF. We're going to generate embeddings for each one of the pages of that PDF. And the reason we generate these embeddings is so we can later compare those embeddings with the question that the user is asking and find the embeddings that are the most similar or the pages of the document that are the most similar to the question the user asks. And I know I'm waving my hands here a little bit in the video that I mentioned before in the other video, which is it's called building a rack system from scratch. On that video, I go into a lot of details about how embeddings work. Hopefully by now, you know that, if not, just check that video out because it's going to help you understand what is the reason we create these embeddings. The good news is that all of these embeddings are going to be created for us behind the scenes and the doc, the the vector store in memory is going to help us retrieve the pages that are the most relevant to answer a specific question. So how do we do that? Well, first, I need to install a couple of libraries here. The first one is going to be DocArray. So I'm going to do PIP install DocArray. That is important. The second one is a specific version of Bidentic. But anyway, I'm installing all of these by hand in the description and in this YouTube video, you're going to find a link to the repo with all of these content. So you don't have to do any of these or you don't have to follow through. You can just go directly and grab the content, including all the libraries that you need to install. OK, I think that's it. Now I need something else. I need to install. This or not, let's see. No, this we might not need this. Or we do, I don't know. Let's see. Let's see if we need that or not. OK, so. Let's create. By the way, let me just hide here the. OK, much better. I have a little bit more space. So here is what I'm going to create now. I'm going to use a DocArray memory search. OK, so this DocArray memory search, this is just going to create a vector storing memory. Now, if we were building a real application, we wouldn't be using this vector storing memory. Obviously, we will be using something that has permanent storage, like pine cone or any other vector database out there. But this is good enough for our video here for our purpose. This is just going to do the same thing, but just in memory here in our computer. And the good thing about this DocArray in memory vector store is that we can just load it and create it off of the documents that we generate it. OK, so you can see here that I'm passing the pages of the document that we generated from the PDF. Right. Remember that we here are the pages, not not here. These are the pages of the document. You can see here pages is just an array with every single page. So we can create a vector store directly off of all of those pages. What's going to happen is that all of those pages are going to go into the database. The database is going to generate embeddings for all of those. It's going to save all of that in memory. There is something else that I need to pass and I need to provide the embeddings class. So what class we're going to be using to generate the embeddings. Here's the thing. Every model uses a different model to generate embeddings. So depending on the model that we create, we need to generate embeddings one way or the other. We have here right now. Either a GPT model or an Olamo model. Therefore, we are going to have to generate embeddings in a different way. So let's see embeddings. Let's create a variable. This is not copilot is not being helpful right there. I think the opening embeddings are here. OK, there we go. So we're going to need this class. So if if we're using a GPT model, the embeddings instance is going to be we're going to instantiate embeddings with the opening embeddings model and the Olamo one. The Olamo ones is going to be this one here. So there is an Olamo embeddings, and that is the one that we want to use if. Oops. This is the one that we want to use if we're using. Lana to or mixed role or any other model, let me execute that again. All right, let me go here. All of this is good. All of this is good. Here is a boom. OK, so there is a problem here with the lottery. OK, so I have a problem here with the lottery. Let me restart this just to make sure that is not what's happening. I know I remember the first time I installed this, that I had issues as well with. The vector storing memory vector store, but now that now is working fine. So now I have a vector store here, which is gray. And I think I can do something like. Retrieve, not tell me a joke, but let me retrieve something related to machine learning. Let me see if this works. Oh, maybe like this. There we go. OK, so if I get my vector store and I turn it into a retriever and a retriever is a component of land chain, that will allow you to retrieve information from anywhere. So basically here what I'm doing, just put it in a different way that it's a little bit less convoluted. I'm going to create a retriever off of the vector store. And again, you don't need a vector store for a retriever. You can create a retriever that's going to be using Google searches. You can create a retriever that's going to get information from anywhere. So just in this case, it's just going to come from the vector store. And then it can invoke my retriever and then pass information. And what's going to happen is that that retriever or that vector store is going to return the four top documents that are the most relevant to the concept machine learning. OK, so anything that's relevant to that concept is going to come sorted in order of importance back. And I think I can say there might be a K here. Let's do two. Not here. Somewhere, somewhere there is actually maybe a top. I don't think it's top K, but there is a parameter which I don't remember right now. I will have to look at the documentation. If you wanted to control how many documents are going to come back, you can do that through the retriever. I'm not sure what the name is right now. It doesn't really matter. We can use four. All right. So now we have a retriever. So the idea here is going back to our chain. Let me copy our chain. Down here. So getting back to my chain, I have a prompt, I have a model and I have a parser. And remember that that prompt is expecting a context and a question. OK, that's what I need to pass that prompt. Now, the context is going to come from this retriever. OK, so this is what's going to get a little bit tricky here. The prompt is expecting a map. So imagine that I do this. If I take a map, I'm going to create a map here and I'm going to pass a context. Let's see the name. What was the the name I was given with Santiago? And I pass a question. What is my name? Can I do this or not? I think I'm going to need something like this. OK, can I do invoke? Doesn't work. Let's figure this out. White. OK, oh, this is a runnable. OK. Can I do this actually? No, that's not it. Yeah. I thought that this was going to get turned into a runnable directly. Let's see why this is not working. So it says, I mean, let me recreate this. I mean, I know how to fix the problem. I just don't want to fix it like that. Let me do let me do this. Import operator. No, sorry. From operator import. I didn't get her. And then let's get an item getter and let's do question. And let's pass that to a retriever. Then let's do this. Still doesn't work. Why is why it doesn't work? Let me see the documentation here really quick. And see why that is not working. OK. You know what? Let me just pass this question here. Oh, well. I understand why that doesn't work. I need to pass a question, obviously. Let's do this. Now there's something let me just check here. OK, so I have the retriever. OK. I have a prompt. I have a model. I have my parser. That is working fine. This looks much better here. The question, let me correct. Let me grab the same question that I'm passing. OK, there we go. This is what that was. That was the problem. All right. So I'm going to explain here really quick what was happening here because it's not it's not obvious what was happening here. All right. So. I have my problem, my model, my parser. I need to pass the prompt. I need to pass a context and I need to pass a question to my prompt. OK, so what I want to do here is the context is going to come from a retriever. So I'm going to put this here, organize it in a different way. So it's more obvious. So the context is going to come from a retriever. But that retriever requires the question that I'm invoking this chain with. So they have these we're and there is another way of doing it. But we're using here something that's called the item getter function. And just so you guys understand what the item getter is, if I do item getter and I pass. ABC, now I can call that actually, compiler is being helpful here. There we go. If I do this, I create this item getter with ABC. I can apply that to a function later that becomes a function that when I call it with a dictionary, for example, it's going to return one, two, three. So if I execute this, you get the one, two, three here. So in this particular case, if I if I have item getter of question and I pass it, I pass this dictionary here. Of course, what I'm going to get back is just the question, right? What is machine learning? Because this is the question. OK, so this is the way you sort of like put together this chain. I'm going to expect here the first module is going to be what they call a runnable. So it's basically a component that can run. So a runnable here is going to generate a map because that map is what's going to be passing. We're going to be passing that map with context and question to the prompt. We're going to generate a map and the first value of the first variable of the map, which is context, is going to come from the retriever. But this is a unit here. So I'm basically grabbing the question from the invoke. I'm grabbing the question, piping that question into a retriever. And the output of that is what's going to go into context. And we know what the output of that is going to be because we already did it here. You can see here this is my retriever. And I'm piping or invoking that with a question, and I'm going to get an array of documents. So this context here is nothing else that this array of documents. OK, and then question, which is the second value of the map, is just the value of the question. I'm passing through the question from the invocation. I'm passing it through here to the second component, and that creates my entire chain. So now I need to test this chain and to test this chain. I actually have a bunch of questions that I'm going to be using to test this chain. And here are my questions. What is the purpose of the course? How many hours of live sessions? How many coding assignments? And just a bunch of questions. And now I'm going to go one by one and I'm going to answer those questions. So this is this actually let's do this for questioning questions. Questioning questions. We can invoke the chain saying, hey, this is my question. I'm going to do something else, which is I'm going to print here the name of the question. I'm going to do, hey, this is actually let me do a print question. I'm going to print the name of the question. I'm going to print the answer. So let's do. Let's do something like this answer. And then let's put all of these here inside. I'm going to need to change these two single quotes and then let's just print a new line. And that is going to call, let me give it a try. What is the purpose of the curse? And this is the answer. Okay. Let's see. Let's see. This is the GPT model by the way. Yeah, this is fine. How many hours the program offers 18 hours of life interactive sessions. That is correct. How many coding assignments there are 30 coding assignments. That is correct. Is there a program certificate? Yes, that is correct. What programming language will be used? Python. How much does the program cost? The program costs $450 for lifetime access. So this is correct. That's the GPT model answering questions from my PDF. Let's now change the model to something different. I'm going to go up here and I'm going to do llama two, but I execute and everything here should stay the same. If we did our jobs, everything should work without making any other changes to what we've built. Let's do all of that. By the way, loading the documents and llama two, uh, it's running here on my computer, which is not, I don't have it. I can pick Nvidia GPU. I have my, my, uh, M three laptops. It's pretty good laptop, but obviously with a bigger GPU is going to run much faster. So let's see. Okay. Answer. Look at this is llama two is just sober both. The answer to your question is 18 hours of hands on life training spread over three weeks, but it works. It's correct. How many coding assignments? I don't know the exact number of coding assignments in the program and code it according to the provided document. There are 30 coding assignments. Jesus Christ. It knows where the information is. It just sucks at summarizing it. Is there a program certificate? Yes. What programming language, uh, Python, how much does the program cost based on the context provided the program costs a thousand to join. That is just not true. It is very clear. I don't think there is even a $1,000 mention in the entire page. So that is just a losing edit that completely. So obviously to get this working, by the way, I also have this model. Let me just try this model just for fun. And then I'm going to show you something else before we finish. Uh, I'm going to try and make straw, uh, just running the whole thing here. Uh, one thing that I wanted to mention is that I've had. Obviously this, these models are not as good as the GPT models. I'm here. I'm not even using GPT four GPT four is so much better. Uh, but if you, if you play with the, with the prompts, uh, you can get these models, uh, to do a very good job summarizing stuff. Obviously my, my prompt here is very, very bare bones just for this example. But this, I mean, don't, don't be discouraged because the model doesn't answer correctly some of these questions. Uh, playing with the prompt will go a long way. Uh, yeah. So let's see, it's still running. This is a big model. Uh, it takes a little bit of time to generate all of those embeddings because the mixed role a seven B is huge compared to Lana too. Let's see, uh, did we started answering yet? Not yet. Still invoking this chain. Okay. It's coming. It's coming through 20 seconds just to invoke this question here. There we go. Okay. So now it's going to try to answer all of those questions. Again, this is just a big model. If we go back to, uh, let me see. Mixed role is this one here is the 26 gigabytes. Compare that to the three gigabyte. This is this one here is Lana too. So the 26 gigabyte is, is mixed role. So it takes quite a bit of time on my computer to produce any results. And speaking of, and while that works, speaking of, of, of being slow and whatnot. Uh, I want to show you something else, which is, it's pretty cool. And the first thing is how to stream. So you can use here. I'm invoking my chain and waiting for an answer to come back to display the whole answer, but a trick just to, to, for your users, when you present this information. Could be just streaming answers back. So you can see here, I'm calling the chain that's streaming instead of calling invoke, I'm calling stream with a question. And whenever this finishes, uh, it has answered one, uh, these hallucinated these four hours of live sessions per week. Oh, no, no, no, this is true with two live sessions each lasting two hours. These live sessions take place every Monday and Thursday. So that is true, but it's not returning. Oh, there we go. Look at this. So mixed role is, is doing a multiplication here. So it's saying it's 12 hours and it's ignoring. Oh, there we go. However, the document mentions that there are 18 hours of hands-on live training. Okay. It's just very verbose. Try to do some math. Uh, yeah. It says that the number is not specifying the documents, but it can be unfair that there are at least 30 coding assignments. What? What do you mean? Like the first tell me that you cannot infer it. I mean that, that is not mentioned. And then that you can infer that. No, you just read that. It says 30 coding assignments in my document is models are just, yeah. Okay. Uh, what model will be used is Python. And it says that the document does not provide information on the, on the, on the cost of the program, by the way, the information is there. You saw GPT doing it. Let's go back to GPT here really quick so we can test the streaming. I'm going to show you streaming, uh, something else really quick. And then we are just going to be done with this. Okay. So this is how it works when it answers one question after the other. Right. And you can see, boom, boom, boom, boom. It displays the answers all together. But if we do streaming, look at what's going to happen. Boom, boom, boom. See, let's, let me try to do it again. See how it just sort of like builds on the question. And it's really fast. That's why you barely notice, but it builds on the answer, uh, just because it's streaming out the characters as they are produced by the model. So that's super cool. The other thing that you can do is just batching, uh, which is also super cool. So here I have a bunch of questions and I'm answering those questions one by one. We can also just do batching. So batching basically, I'm just passing, instead of passing just a single question, I'm passing an array of questions. And when I do that, look, what's going to happen. It's just going to take a little bit more time, but boom, it's just going to display all of the answers at the same time. And the good news is that all of these calls are going to be in parallel behind the scenes, so we don't have to wait for one answer in order to ask the next question we can answer. We can ask many questions at the same time. So the overall result is going to be way faster. So all of that is thanks to land chain. So again, the code is going to be down, uh, in the description below. Just make sure you like this video. It's a ton of work that goes into these videos. Make sure you like these videos. I'm going to be creating more videos, but it's your likes. What makes me create more videos. If you guys don't like it, we'll just going to stop creating like these videos. Uh, what you'll learn today, just as the final thing that I need to mention is how to use these models locally. Right. And you can do these on your Linux server, on your own computer or whatever. Use these models locally and combine them or create a piece of code that will allow you to use these models, uh, regardless of the exact model, you can use one or the order and the entirety of your code does not need to change to reflect that. So hopefully you enjoyed it. Uh, I have a bunch of videos that are going to be coming through. I think the next one is going to be a simpler one. How, instead of just doing a PDF, how you can connect to the web directly and answer questions from a website. I think that's the one that's going to come next. We'll see. Uh, but anyway, thank you. And I will see you in the next one. Bye bye.